# Paper Reading
## Methodology
- [Learning Deep Features for Discriminative Localization](https://arxiv.org/pdf/1512.04150.pdf)
- Figure 2
    - <img src="https://user-images.githubusercontent.com/67457712/230727934-1e4df288-6eca-48d0-9d04-9b6512266d0c.png" width="700">
    - For a given image, let $f_{k}(x, y)$ represent the activation of unit $k$ in the last convolutional layer at spatial location $(x, y)$. Then, for unit $k$, the result of performing global average pooling, $F_{k}$ is $\sum_{x, y}f_{k}(x, y)$. Thus, for a given class $c$, the input to the softmax, $S^{c}$, is $\sum_{k}w^{c}_{k}F_{k}$ where $w^{c}_{k}$ is the weight corresponding to class $c$ for unit $k$. Essentially, $w^{c}_{k}$ indicates the importance of $F_{k}$ for class $c$.
    - ***We explicitly set the input bias of the softmax to 0 as it has little to no impact on the classification performance.***
- Weakly-supervised object localization
    - To generate a bounding box from the CAMs, we use a simple thresholding technique to segment the heatmap. ***We first segment the regions of which the value is above 20% of the max value of the CAM. Then we take the bounding box that covers the largest connected component in the segmentation map. We do this for each of the top-5 predicted classes for the top-5 localization evaluation metric.***
## References


- We believe that GAP loss encourages the net- work to identify the extent of the object as compared to GMP which encourages it to identify just one discrimina- tive part. This is because, when doing the average of a map, the value can be maximized by finding all discriminative parts of an object as all low activations reduce the output of the particular map. On the other hand, for GMP, low scores for all image regions except the most discriminative one do not impact the score as you just perform a max.
- For our experiments we evaluate the effect of using CAM on the following popular CNNs: AlexNet [10], VG- Gnet [23], and GoogLeNet [24]. In general, for each of these networks we remove the fully-connected layers be- fore the final output and replace them with GAP followed by a fully-connected softmax layer.
- We found that the localization ability of the networks im- proved when the last convolutional layer before GAP had a higher spatial resolution, which we term the mapping reso- lution. In order to do this, we removed several convolutional layers from some of the networks. Specifically, we made the following modifications: For AlexNet, we removed the layers after conv5 (i.e., pool5 to prob) resulting in a mapping resolution of 13 × 13. For VGGnet, we removed the layers after conv5-3 (i.e., pool5 to prob), resulting in a mapping resolution of 14 × 14. For GoogLeNet, we removed the layers after inception4e (i.e., pool4 to prob), resulting in a mapping resolution of 14 × 14. To each of the above networks, we added a convolutional layer of size 3 × 3, stride 1, pad 1 with 1024 units, followed by a GAP layer and a softmax layer.